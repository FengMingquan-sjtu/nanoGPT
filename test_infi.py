import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    AutoConfig,  # <-- ç”¨äºŽåŠ è½½æ¨¡åž‹é…ç½®
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset, Dataset, concatenate_datasets
import numpy as np
import os
import shutil
from tqdm import tqdm

# --- 1. é…ç½®å’Œå‡†å¤‡ ---
# æ¨¡åž‹å’Œæ•°æ®é›†é…ç½®
model_name = "distilgpt2"
dataset_name = "wikitext"
dataset_config = "wikitext-2-raw-v1"
max_length = 128  # å‡å°é•¿åº¦ä»¥åŠ å¿«è’¸é¦æ•°æ®ç”Ÿæˆé€Ÿåº¦
train_subset_size = 10000 # æ¨¡æ‹Ÿæ•°æ®å—é™
eval_subset_size = 500

# é›†æˆå­¦ä¹ é…ç½®
num_ensemble_models = 3 # K=3

# è’¸é¦é…ç½®
num_synthetic_samples = 2000 # ç”Ÿæˆçš„åˆæˆæ•°æ®é‡

# --- 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ® ---
print("--- 1. Loading and Preparing Data ---")
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.bos_token = tokenizer.eos_token # distilgpt2æ²¡æœ‰bos_tokenï¼Œç”¨eosä»£æ›¿

# åŠ è½½æ•°æ®é›†
raw_datasets = load_dataset(dataset_name, dataset_config)
raw_datasets["train"] = raw_datasets["train"].shuffle(seed=42).select(range(train_subset_size))
raw_datasets["validation"] = raw_datasets["validation"].shuffle(seed=42).select(range(eval_subset_size)) # ä½¿ç”¨validationé›†ä½œä¸ºæµ‹è¯•é›†

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=max_length)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=["text"])

def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    total_length = (total_length // max_length) * max_length
    result = {
        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

lm_datasets = tokenized_datasets.map(group_texts, batched=True)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# --- 3. è®­ç»ƒå•ä¸ªæ¨¡åž‹ (ä»Žå¤´å¼€å§‹) ---
print("\n--- 2. Training a Single 'Regularized' Model (from scratch) ---")
regularized_model_dir = "regularized_model_from_scratch"
if os.path.exists(regularized_model_dir): shutil.rmtree(regularized_model_dir)

# åŠ è½½æ¨¡åž‹é…ç½®ï¼Œè€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡
config = AutoConfig.from_pretrained(model_name)

training_args = TrainingArguments(
    output_dir=regularized_model_dir,
    overwrite_output_dir=True,
    num_train_epochs=5,  # ä»Žå¤´è®­ç»ƒéœ€è¦æ›´å¤šè½®æ¬¡
    per_device_train_batch_size=8,
    weight_decay=0.8,
    learning_rate=1e-4, # ä»Žå¤´è®­ç»ƒå¯ä»¥ç”¨ç¨é«˜çš„å­¦ä¹ çŽ‡
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    logging_steps=100,
    fp16=True,
)

# ä»Žé…ç½®åˆå§‹åŒ–æ¨¡åž‹ï¼Œæƒé‡æ˜¯éšæœºçš„
model_from_scratch = AutoModelForCausalLM.from_config(config)

trainer = Trainer(
    model=model_from_scratch,
    args=training_args,
    train_dataset=lm_datasets["train"],
    eval_dataset=lm_datasets["validation"],
    data_collator=data_collator,
)
trainer.train()
trainer.save_model(regularized_model_dir)

# --- 4. è®­ç»ƒ K ä¸ªç‹¬ç«‹æ¨¡åž‹ (ä»Žå¤´å¼€å§‹) ---
print(f"\n--- 3. Training {num_ensemble_models} Independent Models (from scratch) ---")
ensemble_model_dirs = []
for i in range(num_ensemble_models):
    output_dir = f"ensemble_model_from_scratch_{i}"
    ensemble_model_dirs.append(output_dir)
    print(f"\n--- Training model {i+1}/{num_ensemble_models} ---")
    if os.path.exists(output_dir): shutil.rmtree(output_dir)

    ensemble_training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=5,
        per_device_train_batch_size=8,
        weight_decay=0.8,
        learning_rate=1e-4,
        seed=42 + i,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        logging_steps=100,
        fp16=True,
    )
    
    # æ¯æ¬¡éƒ½ä»Žéšæœºæƒé‡å¼€å§‹
    ensemble_model = AutoModelForCausalLM.from_config(config)
    trainer = Trainer(
        model=ensemble_model,
        args=ensemble_training_args,
        train_dataset=lm_datasets["train"],
        eval_dataset=lm_datasets["validation"],
        data_collator=data_collator,
    )
    trainer.train()
    trainer.save_model(output_dir)

# --- 5. å®šä¹‰å’Œè¯„ä¼°é›†æˆæ¨¡åž‹ ---
class EnsembleModel(nn.Module):
    def __init__(self, model_dirs, config):
        super().__init__()
        self.models = nn.ModuleList(
            [AutoModelForCausalLM.from_pretrained(d) for d in model_dirs]
        )
        self.config = config # å­˜å‚¨configä»¥å¤‡åŽç”¨

    def forward(self, input_ids, attention_mask=None, labels=None):
        all_logits = []
        for model in self.models:
            # ç¡®ä¿æ¨¡åž‹åœ¨è¯„ä¼°æ¨¡å¼
            model.eval()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            all_logits.append(outputs.logits)
        
        avg_logits = torch.stack(all_logits).mean(dim=0)
        
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = avg_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))
            
        return {"loss": loss, "logits": avg_logits}

# --- 6. é›†æˆè’¸é¦ ---
print("\n--- 4. Starting Ensemble Distillation ---")

# a. åŠ è½½æ•™å¸ˆæ¨¡åž‹
print("   a. Loading the trained ensemble model as the 'Teacher'...")
teacher_model = EnsembleModel(ensemble_model_dirs, config)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
teacher_model.to(device)
teacher_model.eval()

# b. ç”Ÿæˆåˆæˆæ•°æ®
print(f"   b. Generating {num_synthetic_samples} synthetic samples from the Teacher...")
def generate_synthetic_data(teacher, tokenizer, num_samples, max_len):
    synthetic_texts = []
    # ä½¿ç”¨BOS tokenä½œä¸ºç”Ÿæˆçš„èµ·ç‚¹
    start_token_id = tokenizer.bos_token_id
    
    # æ‰¹é‡ç”Ÿæˆä»¥æé«˜æ•ˆçŽ‡
    batch_size = 16
    for _ in tqdm(range(0, num_samples, batch_size)):
        current_batch_size = min(batch_size, num_samples - len(synthetic_texts))
        input_ids = torch.full((current_batch_size, 1), start_token_id, dtype=torch.long, device=device)

        with torch.no_grad():
            for _ in range(max_len - 1):
                outputs = teacher(input_ids)
                next_token_logits = outputs['logits'][:, -1, :]
                
                # ä½¿ç”¨æ¸©åº¦è¿›è¡Œé‡‡æ ·ï¼Œå¢žåŠ å¤šæ ·æ€§
                temperature = 0.9
                next_token_logits = next_token_logits / temperature
                probs = torch.nn.functional.softmax(next_token_logits, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1)
                
                input_ids = torch.cat([input_ids, next_tokens], dim=-1)

        # è§£ç ç”Ÿæˆçš„æ ·æœ¬
        generated_ids = input_ids.cpu().tolist()
        batch_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        synthetic_texts.extend(batch_texts)
        
    return synthetic_texts

synthetic_texts = generate_synthetic_data(teacher_model, tokenizer, num_synthetic_samples, max_length)
synthetic_dataset = Dataset.from_dict({"text": synthetic_texts})

# c. æ··åˆçœŸå®žæ•°æ®å’Œåˆæˆæ•°æ®
print("   c. Mixing real and synthetic data for student training...")
# Tokenizeå’Œå—åŒ–åˆæˆæ•°æ®
tokenized_synthetic = synthetic_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
lm_synthetic = tokenized_synthetic.map(group_texts, batched=True)

# åˆå¹¶æ•°æ®é›†
student_train_dataset = concatenate_datasets([lm_datasets["train"], lm_synthetic])
student_train_dataset = student_train_dataset.shuffle(seed=42)

# d. è®­ç»ƒå­¦ç”Ÿæ¨¡åž‹
print("   d. Training the 'Student' model from scratch...")
student_model_dir = "student_model"
if os.path.exists(student_model_dir): shutil.rmtree(student_model_dir)

# è®ºæ–‡æåˆ°è’¸é¦æ—¶æ­£åˆ™åŒ–å¯ä»¥å¼±ä¸€äº›
student_training_args = TrainingArguments(
    output_dir=student_model_dir,
    overwrite_output_dir=True,
    num_train_epochs=3, # åœ¨æ›´å¤šæ•°æ®ä¸Šï¼Œè½®æ¬¡å¯ä»¥å°‘ä¸€äº›
    per_device_train_batch_size=8,
    weight_decay=0.1, # ä½¿ç”¨è¾ƒå°çš„æƒé‡è¡°å‡
    learning_rate=5e-5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    logging_steps=100,
    fp16=True,
)

# å­¦ç”Ÿæ¨¡åž‹ä¹Ÿä»Žå¤´å¼€å§‹
student_model = AutoModelForCausalLM.from_config(config)

student_trainer = Trainer(
    model=student_model,
    args=student_training_args,
    train_dataset=student_train_dataset,
    eval_dataset=lm_datasets["validation"],
    data_collator=data_collator,
)
student_trainer.train()
student_trainer.save_model(student_model_dir)

# --- 7. æœ€ç»ˆè¯„ä¼°å’Œå¯¹æ¯” ---
print("\n--- 5. Final Evaluation and Comparison ---")

def evaluate_perplexity(model_or_dir, dataset, is_ensemble=False):
    """ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°"""
    if is_ensemble:
        model = model_or_dir
    else:
        model = AutoModelForCausalLM.from_pretrained(model_or_dir)
        model.config = config # ç¡®ä¿æœ‰config
        
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    dataloader = DataLoader(dataset, batch_size=8, collate_fn=data_collator)
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in dataloader:
            inputs = {k: v.to(device) for k, v in batch.items() if k != "labels"}
            labels = batch["labels"].to(device)
            
            if is_ensemble:
                 outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs.get("attention_mask"), labels=labels)
            else:
                 outputs = model(**inputs, labels=labels)

            total_loss += outputs.loss.item()
            num_batches += 1
    
    avg_loss = total_loss / num_batches
    perplexity = np.exp(avg_loss)
    return perplexity

test_dataset = lm_datasets["validation"]

# è¯„ä¼°å•ä¸ªæ­£åˆ™åŒ–æ¨¡åž‹
print("\nEvaluating the single 'Regularized' model...")
regularized_perplexity = evaluate_perplexity(regularized_model_dir, test_dataset)

# è¯„ä¼°é›†æˆæ¨¡åž‹
print("Evaluating the Ensemble model...")
ensemble_perplexity = evaluate_perplexity(teacher_model, test_dataset, is_ensemble=True)

# è¯„ä¼°è’¸é¦åŽçš„å­¦ç”Ÿæ¨¡åž‹
print("Evaluating the distilled 'Student' model...")
student_perplexity = evaluate_perplexity(student_model_dir, test_dataset)


print("\n\n--- FINAL RESULTS ---")
print(f"  - Single Regularized Model Perplexity: {regularized_perplexity:.4f}")
print(f"  - Ensemble Model (K={num_ensemble_models}) Perplexity:   {ensemble_perplexity:.4f}")
print(f"  - Distilled Student Model Perplexity:  {student_perplexity:.4f}")
print("-" * 25)

# ç†æƒ³ç»“æžœ: Ensemble < Student < Single
if ensemble_perplexity < student_perplexity < regularized_perplexity:
    print("\nðŸŽ‰ðŸŽ‰ðŸŽ‰ Perfect Success! The distilled student model retained a significant portion of the ensemble's benefit.")
elif ensemble_perplexity < student_perplexity and student_perplexity < regularized_perplexity:
    print("\nðŸŽ‰ Success! The student model outperformed the single model, demonstrating effective distillation.")
else:
    print("\nðŸ¤” The results did not follow the expected order. This can happen at a small scale. Try increasing data/model size or epochs.")

